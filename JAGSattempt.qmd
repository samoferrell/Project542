---
title: "Fire Determinant Project"
format: html
editor: visual
---

```{r}
knitr::opts_chunk$set(echo = TRUE)

Sys.setenv(JAGS_HOME="C:/Program Files/JAGS/JAGS-4.3.1")

library(rjags)

library(MASS)
library(tidyverse)
data <- read_csv("actual_data_merge (1).csv")

rowsums <- data |>
  select(27:40) |>
  mutate(total = rowSums(across(everything()), na.rm = TRUE))

data$total_fires <- rowsums$total
# 
# 
# 
# 
# data2 <-  data |>
#   select(-1) |>
#   pivot_longer(cols = colnames(data[,27:40]), names_to = "YearMonth", values_to = "Occurrence" )
```

Adding new variable - takes on 1 if a fire occured, 0 if not:

```{r}
bernoulli <- data |>
  mutate(fire = ifelse(total_fires > 0,1,0))
#View(bernoulli)
```

Trying with bernoulli data:

# Data Analysis
```{r}
library(ggplot2)
```
## Tables
Count of fire occurrence by region
```{r}
region_fire <- table(bernoulli$Region, bernoulli$fire)
dimnames(region_fire) <- list(
  "Region" = c("1", "2", "3"),
  "Occurence of at least one fire" = c("No", "Yes"))
region_fire
```
Region and Category counts
```{r}
data3 <- bernoulli
table(data3$Region, data3$Category, dnn=c("Region","Name of Category"))
```
## Bar Graphs
```{r}
df <- as.data.frame(region_fire)
ggplot(df, aes(x = Region, y = Freq, fill = Occurence.of.at.least.one.fire)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Occurrence of Fire by Region",
       x = "Region",
       y = "Count",
       fill = "Occurrence of Fire") +
    scale_fill_manual(values = c("No" = "skyblue", "Yes" = "maroon")) 

```

Region 1 appears to have the most fires. We should note that this graph looks skewed because

Percentage of yes with respect to each region:
```{r}
library(tidyverse)
percent <- bernoulli |>
  group_by(Region) |>
  mutate (Region = as.factor(Region)) |>
  summarize(percent = sum(fire)/ n() * 100)
```

```{r}
ggplot(percent, aes(x = Region, y = percent, fill = Region)) +
  geom_bar(stat = "identity") +
  labs(title = "Percentage of Relative Fire Occurences by Region",
       x = "Region",
       y = "Percentage") 
```
## Plots
Average hectacre of burned area, grouped by region, over time. 
```{r}
filtered <- bernoulli |>
  group_by(Region) |>
  filter(fire == "1") |>
  select(Region,10:24)
```

Pivoting
```{r}
pivot <- filtered |>
  pivot_longer(
    cols = starts_with("AUG") | starts_with("JUL") | starts_with("SEP"),
    names_to = "month_year",
    values_to = "area_burned") |>
    mutate(year = substr(month_year, nchar(month_year) - 3, nchar(month_year)),
           monthpre = substr(month_year, 1, 3),
           # reordering month for plots
           month = factor(monthpre, levels = c("JUL","AUG","SEP")),
           
    Region = as.factor(Region))
```

```{r}
month_summary <- pivot |>
  group_by(Region,year,month) |>
  summarize(mean_area_burned = (mean(area_burned, na.rm = TRUE)))

```

```{r}
year_summary <- pivot |>
    group_by(Region,year) |>
    summarize(mean_area_burned = (mean(area_burned, na.rm = TRUE)))
```


```{r}
head(year_summary)
head(month_summary)
```
```{r}
ggplot(year_summary, aes(x = year, y = mean_area_burned, 
                         color = Region, group = Region)) +
  geom_point(size = 3) +
  geom_line() +
  labs(title = "Average Area Burned per Year, Grouped by Region")
```



```{r}
ggplot(month_summary, aes(x = month, y = mean_area_burned, 
                         color = year, group = year)) +
  geom_point(size = 3) +
  geom_line() +
  facet_grid(year ~ Region) +
  labs(title = "Average Area Burned per Month, Grouped by Year and Region")
```




## Boxplots

```{r}
pivot_filtered <- pivot |>
  filter(!is.na(area_burned))
ggplot(pivot_filtered, aes(x = Region, y = area_burned, fill = Region)) +
  geom_boxplot() +
  facet_wrap(~ year) 
```


Outlier removal - source: https://sqlpad.io/tutorial/remove-outliers/
```{r}
removeOutliers <- function(data, column_name) {
  Q1 <- quantile(data[[column_name]], 0.25)
  Q3 <- quantile(data[[column_name]], 0.75)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  return(data[data[[column_name]] >= lower_bound & data[[column_name]] <= upper_bound, ])
}
```

Applying function:
```{r}
boxplot_no_outliers <- removeOutliers(data = pivot_filtered, column_name = "area_burned")
```

```{r}
ggplot(boxplot_no_outliers, aes(x = Region, y = area_burned, fill = Region)) +
  geom_boxplot() +
  facet_wrap(~ year) 
```

```{r}
ggplot(boxplot_no_outliers, aes(x = month, y = area_burned, fill = month)) +
  geom_boxplot()
```

```{r}
ggplot(boxplot_no_outliers, aes(x = area_burned)) +
  geom_histogram(
    bins = 20,               
    fill = "lightblue",   
    color = "black") +
  labs(title = "Histogram of Area Burned", 
       x = "Area Burned",                       
       y = "Frequency") +
 facet_wrap(~ month)
```


# Models

## Only main effect for all terms
```{r}

model_string <- textConnection("model{
    for (i in 1:length(y)) {
      y[i] ~ dbern(p[i])
      logit(p[i]) = beta0 + beta1*x1[i] + beta2*x2[i] + beta3*x3[i] + beta4*x4[i] 
    }
    #Uninformative Priors:
    beta0 ~ dnorm(0 ,1/(10)^2 )
    beta1 ~ dnorm(0, 1/(10)^2)
    beta2 ~ dnorm(0, 1/(10)^2)
    beta3 ~ dnorm(0, 1/(10)^2)
    beta4 ~ dnorm(0, 1/(10)^2)

}")

# Dr. Reich does not standardize the data, but other resource does


data_jags = list(y=bernoulli$fire, x1 = bernoulli$NEAR_DIST, x2 = bernoulli$Shape_Leng, x3 = bernoulli$Shape_Area, x4 = bernoulli$Region)

model <- jags.model(model_string,data = data_jags, n.chains=3,quiet=TRUE)
update(model, 100, progress.bar="none")
params  <- c("beta0", "beta1","beta2","beta3","beta4")
samples <- coda.samples(model, 
           variable.names=params, 
           n.iter=1000, progress.bar="none")
summary(samples)

# Compute DIC - n.iter needs to be the same above and below
DIC    <- dic.samples(model,n.iter=1000,n.thin = 5, progress.bar="none")
DIC
```

CV attempt
```{r}
model_string <- "model {
    for (i in 1:length(y)) {
        y[i] ~ dbern(p[i])
        logit(p[i]) = beta0 + beta1 * x1[i] + beta2 * x2[i] + beta3 * x3[i] + beta4 * x4[i]
    }
    beta0 ~ dnorm(0, 1/(10)^2)
    beta1 ~ dnorm(0, 1/(10)^2)
    beta2 ~ dnorm(0, 1/(10)^2)
    beta3 ~ dnorm(0, 1/(10)^2)
    beta4 ~ dnorm(0, 1/(10)^2)
}
"

# Number of folds for cross-validation
k <- 5
n <- nrow(bernoulli)
fold_size <- floor(n / k)

# Initialize vectors to store results
predictions <- vector("list", k)
true_values <- vector("list", k)

# Perform k-fold cross-validation
# Perform k-fold cross-validation
for (fold in 1:k) {
    # ... Same data splitting and model fitting code as before ...
    
    # Extract samples for this fold
    samples_fold <- samples[, , fold]  # Extract samples from all chains for this fold
    
    # Extract posterior means or medians (point estimates)
    beta0_samples <- samples_fold$beta0
    beta1_samples <- samples_fold$beta1
    beta2_samples <- samples_fold$beta2
    beta3_samples <- samples_fold$beta3
    beta4_samples <- samples_fold$beta4
    
    # Calculate predicted probabilities for the test set
    # Assuming logistic regression equation here
    logit_p <- beta0_samples + beta1_samples * data_jags_test$x1 +
               beta2_samples * data_jags_test$x2 +
               beta3_samples * data_jags_test$x3 +
               beta4_samples * data_jags_test$x4
    
    p <- 1 / (1 + exp(-logit_p))  # Convert logit to probability
    
    # Store predictions and true values
    predictions[[fold]] <- p
    true_values[[fold]] <- bernoulli$fire[test_indices]
    
    # Clean up
    closeAllConnections()
}
```


```{r}
b0 <- c(samples[[1]][,1],samples[[2]][,1])
b1 <- c(samples[[1]][,2],samples[[2]][,2])
b2 <- c(samples[[1]][,3],samples[[2]][,3])
b3 <- c(samples[[1]][,4],samples[[2]][,4])
b4 <- c(samples[[1]][,5],samples[[2]][,5])
```

## Prediction

```{r}
# Combine results from 3 different chains into one by stacking matrices that contain simulations.
mod1_csim = as.mcmc(do.call(rbind, samples))

# Extract posterior mean of coefficients
pm_coef = colMeans(mod1_csim)

# The matrix multiplication below gives the exponentiation part in equation which will then be used to find estimated probabilities.

predictors <- matrix(c(bernoulli$NEAR_DIST, bernoulli$Shape_Leng, bernoulli$Shape_Area, bernoulli$Region), nrow = 1282, ncol = 4)

pm_Xb = pm_coef["beta0"] + predictors %*% pm_coef[1:4] 
# Intercept + Design Matrix*Coefficients

# extract probability (basically undo logit link)
phat = 1.0 / (1.0 + exp(-pm_Xb))  
# Predicted probabilities that the Outcome = 1 for each observations

plot(phat, jitter(bernoulli$fire))
(tab0.5 = table(phat > 0.5, bernoulli$fire))
```


## Main effects of first three vars

```{r}

model_string2 <- textConnection("model{
    for (i in 1:length(y)) {
      y[i] ~ dbern(p[i])
      logit(p[i]) = beta0 + beta1*x1[i] + beta2*x2[i] + beta3*x3[i]
    }
    #Uninformative Priors:
    beta0 ~ dnorm(0 ,1/(10)^2 )
    beta1 ~ dnorm(0, 1/(10)^2)
    beta2 ~ dnorm(0, 1/(10)^2)
    beta3 ~ dnorm(0, 1/(10)^2)

}")

# Dr. Reich does not standardize the data, but other resource does


data_jags2 = list(y=bernoulli$fire, x1 = bernoulli$NEAR_DIST, x2 = bernoulli$Shape_Leng, x3 = bernoulli$Shape_Area)

model2 <- jags.model(model_string2,data = data_jags2, n.chains=3,quiet=TRUE)
update(model2, 100, progress.bar="none")
params2  <- c("beta0", "beta1","beta2","beta3")
samples2 <- coda.samples(model2, 
           variable.names=params2, 
           n.iter=1000, progress.bar="none")
summary(samples2)
# Compute DIC - n.iter needs to be the same above and below
DIC2    <- dic.samples(model2,n.iter=1000,n.thin = 5, progress.bar="none")
DIC2
```



That raised DIC slightly....  
So let's try model with main effects for NEAR_DIST, shape length, and region

## Main effects of x1, x2, and x4

```{r}

model_string3 <- textConnection("model{
    for (i in 1:length(y)) {
      y[i] ~ dbern(p[i])
      logit(p[i]) = beta0 + beta1*x1[i] + beta2*x2[i] + beta3*x4[i]
    }
    #Uninformative Priors:
    beta0 ~ dnorm(0 ,1/(10)^2 )
    beta1 ~ dnorm(0, 1/(10)^2)
    beta2 ~ dnorm(0, 1/(10)^2)
    beta3 ~ dnorm(0, 1/(10)^2)

}")

# Dr. Reich does not standardize the data, but other resource does


data_jags3 = list(y=bernoulli$fire, x1 = bernoulli$NEAR_DIST, x2 = bernoulli$Shape_Leng, x4 = bernoulli$Region)

model3 <- jags.model(model_string3,data = data_jags3, n.chains=3,quiet=TRUE)
update(model3, 100, progress.bar="none")
params3  <- c("beta0", "beta1","beta2","beta3")
samples3 <- coda.samples(model3, 
           variable.names=params3, 
           n.iter=1000, progress.bar="none")
summary(samples3)
# Compute DIC - n.iter needs to be the same above and below
DIC3    <- dic.samples(model3,n.iter=1000,n.thin = 5, progress.bar="none")
DIC3
```


## Main effects of x1 and x2

```{r}

model_string4 <- textConnection("model{
    for (i in 1:length(y)) {
      y[i] ~ dbern(p[i])
      logit(p[i]) = beta0 + beta1*x1[i] + beta2*x2[i]
    }
    #Uninformative Priors:
    beta0 ~ dnorm(0 ,1/(10)^2 )
    beta1 ~ dnorm(0, 1/(10)^2)
    beta2 ~ dnorm(0, 1/(10)^2)

}")

# Dr. Reich does not standardize the data, but other resource does


data_jags4 = list(y=bernoulli$fire, x1 = bernoulli$NEAR_DIST, x2 = bernoulli$Shape_Leng)

model4 <- jags.model(model_string4,data = data_jags4, n.chains=3,quiet=TRUE)
update(model4, 100, progress.bar="none")
params4  <- c("beta0", "beta1","beta2")
samples4 <- coda.samples(model4, 
           variable.names=params4, 
           n.iter=1000, progress.bar="none")
summary(samples4)
# Compute DIC - n.iter needs to be the same above and below
DIC4    <- dic.samples(model4,n.iter=1000,n.thin = 5, progress.bar="none")
DIC4
```