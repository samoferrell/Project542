---
title: "Fire Determinant Project"
format: html
editor: visual
---

```{r}
knitr::opts_chunk$set(echo = TRUE)

Sys.setenv(JAGS_HOME="C:/Program Files/JAGS/JAGS-4.3.1")

library(rjags)
library(reshape2)
library(coda)
library(MASS)
library(tidyverse)
data <- read_csv("actual_data_merge (1).csv")

rowsums <- data |>
  select(27:40) |>
  mutate(total = rowSums(across(everything()), na.rm = TRUE))

data$total_fires <- rowsums$total
# 
# 
# 
# 
# data2 <-  data |>
#   select(-1) |>
#   pivot_longer(cols = colnames(data[,27:40]), names_to = "YearMonth", values_to = "Occurrence" )
```

Here we will add a new variable called "fire" into the data set. This variable will be a 1 if at least one fire occurred in the area and will be a 0 if no fires occurred.

```{r}
bernoulli <- data |>
  mutate(fire = ifelse(total_fires > 0,1,0))
#View(bernoulli)
```

Now we will analyze this data with the "fire" variable added to the data set.

# Data Analysis

```{r}
library(ggplot2)
```

## Tables

Count of fire occurrence by region

```{r}
region_fire <- table(bernoulli$Region, bernoulli$fire)
dimnames(region_fire) <- list(
  "Region" = c("1", "2", "3"),
  "Occurence of at least one fire" = c("No", "Yes"))
region_fire
```

Region and Category counts

```{r}
data3 <- bernoulli
table(data3$Region, data3$Category, dnn=c("Region","Name of Category"))
```

## Bar Graphs

```{r}
df <- as.data.frame(region_fire)
ggplot(df, aes(x = Region, y = Freq, fill = Occurence.of.at.least.one.fire)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Occurrence of Fire by Region",
       x = "Region",
       y = "Count",
       fill = "Occurrence of Fire") +
    scale_fill_manual(values = c("No" = "skyblue", "Yes" = "maroon")) 

```

Region 1 appears to have the most fires. We should note that this graph looks skewed because

Percentage of yes with respect to each region:

```{r}
library(tidyverse)
percent <- bernoulli |>
  group_by(Region) |>
  mutate (Region = as.factor(Region)) |>
  summarize(percent = sum(fire)/ n() * 100)
```

```{r}
ggplot(percent, aes(x = Region, y = percent, fill = Region)) +
  geom_bar(stat = "identity") +
  labs(title = "Percentage of Relative Fire Occurences by Region",
       x = "Region",
       y = "Percentage") 
```

## Plots

Average hectacre of burned area, grouped by region, over time.

```{r}
filtered <- bernoulli |>
  group_by(Region) |>
  filter(fire == "1") |>
  select(Region,10:24)
```

Pivoting

```{r}
pivot <- filtered |>
  pivot_longer(
    cols = starts_with("AUG") | starts_with("JUL") | starts_with("SEP"),
    names_to = "month_year",
    values_to = "area_burned") |>
    mutate(year = substr(month_year, nchar(month_year) - 3, nchar(month_year)),
           monthpre = substr(month_year, 1, 3),
           # reordering month for plots
           month = factor(monthpre, levels = c("JUL","AUG","SEP")),
           
    Region = as.factor(Region))
```

```{r}
# Grouping by region, year, and month and filtering each year
month_summary_2019 <- pivot |>
  group_by(Region,year,month) |>
  summarize(mean_area_burned = (mean(area_burned, na.rm = TRUE))) |>
  filter(year == 2019)
month_summary_2020 <- pivot |>
  group_by(Region,year,month) |>
  summarize(mean_area_burned = (mean(area_burned, na.rm = TRUE))) |>
  filter(year == 2020)
month_summary_2021 <- pivot |>
  group_by(Region,year,month) |>
  summarize(mean_area_burned = (mean(area_burned, na.rm = TRUE))) |>
  filter(year == 2021)
month_summary_2022 <- pivot |>
  group_by(Region,year,month) |>
  summarize(mean_area_burned = (mean(area_burned, na.rm = TRUE))) |>
  filter(year == 2022)
month_summary_2023 <- pivot |>
  group_by(Region,year,month) |>
  summarize(mean_area_burned = (mean(area_burned, na.rm = TRUE))) |>
  filter(year == 2023)
```

```{r}
year_summary <- pivot |>
    group_by(Region,year) |>
    summarize(mean_area_burned = (mean(area_burned, na.rm = TRUE)))
```

For this table, we will look at the average area burned each year in each region, averaged over all 3 months.

```{r}
stargazer(year_summary, type = "text", title = "Averaged Area Burned for Each Year Through All Months by Region", digits = 2, summary = FALSE, rownames = FALSE, out = "yearaverages.txt")
```

Next, we will look at the average area burned by month in 2019.

```{r}
stargazer(month_summary_2019, type = "text", title = "Averaged Area Burned per Region and Month in 2019", digits = 2, summary = FALSE, rownames = FALSE, out = "2019table.txt")
```

Here we will look at the average area burned in 2020.

```{r}
stargazer(month_summary_2020, type = "text", title = "Averaged Area Burned per Region and Month in 2020", digits = 2, summary = FALSE, rownames = FALSE, out = "2020table.txt")
```

Next, we will look at 2021, 2022, and 2023.

```{r}
stargazer(month_summary_2021, type = "text", title = "Averaged Area Burned per Region and Month in 2021", digits = 2, summary = FALSE, rownames = FALSE, out = "2021table.txt")
```

```{r}
stargazer(month_summary_2022, type = "text", title = "Averaged Area Burned per Region and Month in 2022", digits = 2, summary = FALSE, rownames = FALSE, out = "2022table.txt")
```

```{r}
stargazer(month_summary_2023, type = "text", title = "Averaged Area Burned per Region and Month in 2023", digits = 2, summary = FALSE, rownames = FALSE, out = "2023table.txt")
```

```{r}
ggplot(year_summary, aes(x = year, y = mean_area_burned, 
                         color = Region, group = Region)) +
  geom_point(size = 3) +
  geom_line() +
  labs(title = "Average Area Burned per Year, Grouped by Region")
```

```{r}
ggplot(month_summary, aes(x = month, y = mean_area_burned, 
                         color = year, group = year)) +
  geom_point(size = 3) +
  geom_line() +
  facet_grid(year ~ Region) +
  labs(title = "Average Area Burned per Month, Grouped by Year and Region")
```

## Boxplots

```{r}
pivot_filtered <- pivot |>
  filter(!is.na(area_burned))
ggplot(pivot_filtered, aes(x = Region, y = area_burned, fill = Region)) +
  geom_boxplot() +
  facet_wrap(~ year) 
```

Outlier removal - source: https://sqlpad.io/tutorial/remove-outliers/

```{r}
removeOutliers <- function(data, column_name) {
  Q1 <- quantile(data[[column_name]], 0.25)
  Q3 <- quantile(data[[column_name]], 0.75)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  return(data[data[[column_name]] >= lower_bound & data[[column_name]] <= upper_bound, ])
}
```

Applying function:

```{r}
boxplot_no_outliers <- removeOutliers(data = pivot_filtered, column_name = "area_burned")
```

```{r}
ggplot(boxplot_no_outliers, aes(x = Region, y = area_burned, fill = Region)) +
  geom_boxplot() +
  facet_wrap(~ year) 
```

```{r}
ggplot(boxplot_no_outliers, aes(x = month, y = area_burned, fill = month)) +
  geom_boxplot()
```

```{r}
ggplot(boxplot_no_outliers, aes(x = area_burned)) +
  geom_histogram(
    bins = 20,               
    fill = "lightblue",   
    color = "black") +
  labs(title = "Histogram of Area Burned", 
       x = "Area Burned",                       
       y = "Frequency") +
 facet_wrap(~ month)
```

# Models

## Only main effect for all terms

```{r}

model_string <- textConnection("model{
    for (i in 1:length(y)) {
      y[i] ~ dbern(p[i])
      logit(p[i]) = beta0 + beta1*x1[i] + beta2*x2[i] + beta3*x3[i] + beta4*x4[i] 
    }
    #Uninformative Priors:
    beta0 ~ dnorm(0 ,1/(10)^2 )
    beta1 ~ dnorm(0, 1/(10)^2)
    beta2 ~ dnorm(0, 1/(10)^2)
    beta3 ~ dnorm(0, 1/(10)^2)
    beta4 ~ dnorm(0, 1/(10)^2)

}")

# Dr. Reich does not standardize the data, but other resource does


data_jags = list(y=bernoulli$fire, x1 = bernoulli$NEAR_DIST, x2 = bernoulli$Shape_Leng, x3 = bernoulli$Shape_Area, x4 = bernoulli$Region)

model <- jags.model(model_string,data = data_jags, n.chains=3,quiet=TRUE)
update(model, 100, progress.bar="none")
params  <- c("beta0", "beta1","beta2","beta3","beta4")
samples <- coda.samples(model, 
           variable.names=params, 
           n.iter=1000, progress.bar="none")
summary(samples)

# Compute DIC - n.iter needs to be the same above and below
DIC    <- dic.samples(model,n.iter=1000,n.thin = 5, progress.bar="none")
DIC
```

<<<<<<< HEAD
CV attempt
```{r}
model_string <- "model {
    for (i in 1:length(y)) {
        y[i] ~ dbern(p[i])
        logit(p[i]) = beta0 + beta1 * x1[i] + beta2 * x2[i] + beta3 * x3[i] + beta4 * x4[i]
    }
    beta0 ~ dnorm(0, 1/(10)^2)
    beta1 ~ dnorm(0, 1/(10)^2)
    beta2 ~ dnorm(0, 1/(10)^2)
    beta3 ~ dnorm(0, 1/(10)^2)
    beta4 ~ dnorm(0, 1/(10)^2)
}
"

# Number of folds for cross-validation
k <- 5
n <- nrow(bernoulli)
fold_size <- floor(n / k)

# Initialize vectors to store results
predictions <- vector("list", k)
true_values <- vector("list", k)

# Perform k-fold cross-validation
# Perform k-fold cross-validation
for (fold in 1:k) {
    # ... Same data splitting and model fitting code as before ...
    
    # Extract samples for this fold
    samples_fold <- samples[, , fold]  # Extract samples from all chains for this fold
    
    # Extract posterior means or medians (point estimates)
    beta0_samples <- samples_fold$beta0
    beta1_samples <- samples_fold$beta1
    beta2_samples <- samples_fold$beta2
    beta3_samples <- samples_fold$beta3
    beta4_samples <- samples_fold$beta4
    
    # Calculate predicted probabilities for the test set
    # Assuming logistic regression equation here
    logit_p <- beta0_samples + beta1_samples * data_jags_test$x1 +
               beta2_samples * data_jags_test$x2 +
               beta3_samples * data_jags_test$x3 +
               beta4_samples * data_jags_test$x4
    
    p <- 1 / (1 + exp(-logit_p))  # Convert logit to probability
    
    # Store predictions and true values
    predictions[[fold]] <- p
    true_values[[fold]] <- bernoulli$fire[test_indices]
    
    # Clean up
    closeAllConnections()
}
```


=======
>>>>>>> 290f863836f4f1ad24ab364af183b43ddf2bafcf
```{r}
b0 <- c(samples[[1]][,1],samples[[2]][,1])
b1 <- c(samples[[1]][,2],samples[[2]][,2])
b2 <- c(samples[[1]][,3],samples[[2]][,3])
b3 <- c(samples[[1]][,4],samples[[2]][,4])
b4 <- c(samples[[1]][,5],samples[[2]][,5])
```

## Prediction

```{r}
# Combine results from 3 different chains into one by stacking matrices that contain simulations.
mod1_csim = as.mcmc(do.call(rbind, samples))

# Extract posterior mean of coefficients
pm_coef = colMeans(mod1_csim)

# The matrix multiplication below gives the exponentiation part in equation which will then be used to find estimated probabilities.

predictors <- matrix(c(bernoulli$NEAR_DIST, bernoulli$Shape_Leng, bernoulli$Shape_Area, bernoulli$Region), nrow = 1282, ncol = 4)

pm_Xb = pm_coef["beta0"] + predictors %*% pm_coef[1:4] 
# Intercept + Design Matrix*Coefficients

# extract probability (basically undo logit link)
phat = 1.0 / (1.0 + exp(-pm_Xb))  
# Predicted probabilities that the Outcome = 1 for each observations

plot(phat, jitter(bernoulli$fire))
(tab0.5 = table(phat > 0.5, bernoulli$fire))
```

## Main effects of first three vars

```{r}

model_string2 <- textConnection("model{
    for (i in 1:length(y)) {
      y[i] ~ dbern(p[i])
      logit(p[i]) = beta0 + beta1*x1[i] + beta2*x2[i] + beta3*x3[i]
    }
    #Uninformative Priors:
    beta0 ~ dnorm(0 ,1/(10)^2 )
    beta1 ~ dnorm(0, 1/(10)^2)
    beta2 ~ dnorm(0, 1/(10)^2)
    beta3 ~ dnorm(0, 1/(10)^2)

}")

# Dr. Reich does not standardize the data, but other resource does


data_jags2 = list(y=bernoulli$fire, x1 = bernoulli$NEAR_DIST, x2 = bernoulli$Shape_Leng, x3 = bernoulli$Shape_Area)

model2 <- jags.model(model_string2,data = data_jags2, n.chains=3,quiet=TRUE)
update(model2, 100, progress.bar="none")
params2  <- c("beta0", "beta1","beta2","beta3")
samples2 <- coda.samples(model2, 
           variable.names=params2, 
           n.iter=1000, progress.bar="none")
summary(samples2)
# Compute DIC - n.iter needs to be the same above and below
DIC2    <- dic.samples(model2,n.iter=1000,n.thin = 5, progress.bar="none")
DIC2
```

That raised DIC slightly....\
So let's try model with main effects for NEAR_DIST, shape length, and region

## Main effects of x1, x2, and x4

```{r}

model_string3 <- textConnection("model{
    for (i in 1:length(y)) {
      y[i] ~ dbern(p[i])
      logit(p[i]) = beta0 + beta1*x1[i] + beta2*x2[i] + beta3*x4[i]
    }
    #Uninformative Priors:
    beta0 ~ dnorm(0 ,1/(10)^2 )
    beta1 ~ dnorm(0, 1/(10)^2)
    beta2 ~ dnorm(0, 1/(10)^2)
    beta3 ~ dnorm(0, 1/(10)^2)

}")

# Dr. Reich does not standardize the data, but other resource does


data_jags3 = list(y=bernoulli$fire, x1 = bernoulli$NEAR_DIST, x2 = bernoulli$Shape_Leng, x4 = bernoulli$Region)

model3 <- jags.model(model_string3,data = data_jags3, n.chains=3,quiet=TRUE)
update(model3, 100, progress.bar="none")
params3  <- c("beta0", "beta1","beta2","beta3")
samples3 <- coda.samples(model3, 
           variable.names=params3, 
           n.iter=1000, progress.bar="none")
summary(samples3)
# Compute DIC - n.iter needs to be the same above and below
DIC3    <- dic.samples(model3,n.iter=1000,n.thin = 5, progress.bar="none")
DIC3
```

## Main effects of x1 and x2

```{r}
set.seed(50)
model_string4 <- textConnection("model{
    for (i in 1:length(y)) {
      y[i] ~ dbern(p[i])
      logit(p[i]) = beta0 + beta1*x1[i] + beta2*x2[i]
    }
    #Uninformative Priors:
    beta0 ~ dnorm(0 ,1/(10)^2)
    beta1 ~ dnorm(0, 1/(10)^2)
    beta2 ~ dnorm(0, 1/(10)^2)
}")

# Dr. Reich does not standardize the data, but other resource does


data_jags4 = list(y=bernoulli$fire, x1 = bernoulli$NEAR_DIST, x2 = bernoulli$Shape_Leng)

model4 <- jags.model(model_string4,data = data_jags4, n.chains=3,quiet=TRUE)
update(model4, 100, progress.bar="none")
params4  <- c("beta0", "beta1","beta2")
samples4 <- coda.samples(model4, 
           variable.names=params4, 
           n.iter=1000, progress.bar="none")
summary(samples4)
# Compute DIC - n.iter needs to be the same above and below
DIC4    <- dic.samples(model4,n.iter=1000,n.thin = 5, progress.bar="none")
DIC4
```

## Adding an Interaction Term into Model

```{r}
set.seed(50)
model_string5 <- textConnection("model{
    for (i in 1:length(y)) {
      y[i] ~ dbern(p[i])
      logit(p[i]) = beta0 + beta1*x1[i] + beta2*x2[i] + beta3*x3[i]
    }
    #Uninformative Priors:
    beta0 ~ dnorm(0 ,1/(10)^2)
    beta1 ~ dnorm(0, 1/(10)^2)
    beta2 ~ dnorm(0, 1/(10)^2)
    beta3 ~ dnorm(0, 1/(10)^2)

}")

# Dr. Reich does not standardize the data, but other resource does


data_jags5 = list(y=bernoulli$fire, x1 = bernoulli$NEAR_DIST, x2 = bernoulli$Shape_Leng, x3 = bernoulli$Shape_Leng*bernoulli$NEAR_DIST)

model5 <- jags.model(model_string5,data = data_jags5, n.chains=3,quiet=TRUE)
update(model5, 100, progress.bar="none")
params5  <- c("beta0", "beta1","beta2", "beta3")
samples5 <- coda.samples(model5, 
           variable.names=params5, 
           n.iter=1000, progress.bar="none")
summary(samples5)
# Compute DIC - n.iter needs to be the same above and below
DIC5    <- dic.samples(model5,n.iter=1000,n.thin = 5, progress.bar="none")
DIC5

samples_df <- as.data.frame(do.call(rbind, lapply(samples5, as.matrix)))

# Melt the data frame to long format for ggplot2
samples_long <- melt(samples_df)
names(samples_long) <- c("Parameter", "Value")

# Filter to beta3
samples_beta3 <- subset(samples_long, Parameter == "beta3")

# Beta3 Plot
ggplot(samples_beta3, aes(x=Parameter, y=Value)) +
  geom_boxplot() +
  geom_hline(yintercept=0, linetype="dashed", color="red") +
  labs(title="Boxplot of Interaction Term",
       x="Coefficient of Interaction Term",
       y="Value") +
  theme_minimal()

samples_beta1 <- subset(samples_long, Parameter == "beta1")
## Beta1 plot
ggplot(samples_beta1, aes(x=Parameter, y=Value)) +
  geom_boxplot() +
  geom_hline(yintercept=0, linetype="dashed", color="red") +
  labs(title="Boxplot of Distance from Nearest Municipality Slope",
       x="Coefficient of Distance from Nearest Municipality",
       y="Value") +
  theme_minimal()

samples_beta2 <- subset(samples_long, Parameter == "beta2")
## Beta2 plot
ggplot(samples_beta2, aes(x=Parameter, y=Value)) +
  geom_boxplot() +
  geom_hline(yintercept=0, linetype="dashed", color="red") +
  labs(title="Boxplot of Perimeter of Lot Slope",
       x="Perimeter of Lot Slope",
       y="Value") +
  theme_minimal()
```

odds <- exp(.03162)
odds2 <- exp(.0002308)
odds3 <- exp(-.00001478)

## Interpretation of Beta 1

For each increase in hectare away from a municipality, the odds of a fire occurring increases by 3.21% with the other variables in the model held constant.

## Interpretation of Beta 2

For each increase in feet the perimeter of the lot, the odds of a fire occurring increases by .0231% with the other variables in the model held constant.

## Interpretation of Beta 3

For each increase in unit of the interaction between hectares away from a municipality and perimeter of the lot, the odds of a fire occurring decreases by .0015%.

